\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
     \usepackage[nonatbib]{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{chemformula}

\title{Deep Reinforcement Learning to Minimize Long-Term Carbon Emissions and Cost in the Investment of Electricity Generation}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  David S.~Hippocampus\thanks{Use footnote for providing further information
    about author (webpage, alternative address)---\emph{not} for acknowledging
    funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}

A transition from a high-carbon emitting electricity power system to one based on renewables would aid in the mitigation of climate change. Decarbonization of the electricity grid would allow for low-carbon heating, cooling and transport. Investments in renewable energy, however, must be made over a long time horizon, with various uncertainties in future electricity demand and costs. 

To account for imperfect information of the future, we use the deep deterministic policy gradient (DDPG) deep reinforcement learning to optimize for a low-cost, low-carbon electricity supply using a modified version of the FTT:Power model. The DDPG algorithm is able to learn the optimum electricity mix through experience to achieve this between the years of 2017 and 2050. We find that a transition from fossil fuels and nuclear power to renewables, based upon wind, solar and wave would provide a cheap and low-carbon alternative.

\end{abstract}




\section{Introduction}
\label{sec:intro}


A transition from a high carbon electricity supply to a low-carbon system is central to avoiding catastrophic climate change \cite{Kell2020}. A low-carbon electricity supply would aid in the decarbonization of the automotive and heating sectors. Such a transition must be made in a gradual approach to maintain grid reliability \cite{Kahrl2011}.

Renewable energy costs, such as solar and wind energy, have dropped recently, making them cost-competitive with fossil fuels. These drops in prices are projected to continue \cite{IEA2015}. The future cost of generation, demand and fuel prices, however, remain uncertain over the long-term future. These uncertainties are risks which investors must analyze while making long-term investment decisions.

In this paper, we use the deep deterministic policy gradient (DDPG) reinforcement learning algorithm to simulate the behaviour of investors over a 33-year horizon, between 2017 and 2050 \cite{Hunt2016a}. We projected until 2050 due to the fact that this is a frequent target for governments to reach zero carbon. The environment used was a modified version of the FTT:Power model \cite{Mercure2012}. FTT:Power is a global power systems model that uses logistic differential equations to simulate technology switching. 

We modified the FTT:Power model to use the DDPG algorithm in place of the logistic differential equations to simulate investment decisions. In addition to this, we only simulated two countries: the United Kingdom and Ireland. We chose these due to our previous work on modelling the UK electricity mix over a similar horizon \cite{Kell2020}. The DDPG algorithm allowed us to simulate the decisions made by investors under imperfect information over a 33-year period. 

The reinforcement learning algorithm enabled us to model the behaviour of an investor without perfect knowledge of the future, with a view to reducing carbon emissions and the overall cost of the system. The reinforcement learning agent is a single actor that invests in both the UK and Ireland. This work enabled us to see whether a low-carbon mix is possible over the next 33-years to avert climate change.

Oliveira \textit{et al.} also use reinforcement learning for the capacity expansion problem~\cite{Oliveira2018}. They, however, focus on a 20-year time horizon. Kazempour \textit{et al.} use a mixed integer linear programming approach to solve the generation investment problem \cite{Kazempour2011}. In our work, we address a gap in the literature for the capacity expansion problem over a 33 year time period using deep reinforcement learning to reduce both carbon emissions and electricity price.

Through this work, it is possible to assess whether a low cost, low-carbon electricity mix is viable over the long-term using a deep reinforcement learning investment algorithm, as well as finding what this optimum mix should be. Our approach is in contrast to a mixed-integer linear programming problem, where full knowledge of the time-horizon is required. This work enables us to closely match the investment behaviour of rational agents, without knowledge of the future. It can help guide investors on which technologies to invest in over the long-term, as well as the proportions to invest in.


%\begin{itemize}
%	\item Requirement to reduce carbon emissions globally.
%	\item This must be achieved cost effectively.
%	\item Requirement for a global solution to find optimal mix of electricity mix with imperfect information about the future (eg. costs and demand).
%	\item Use of reinforcement learning to take into account all uncertainties to achieve goal of a cost effective and low-carbon solution
%	\item Use of FTT:Power model to simulate investment in the UK and Ireland over a 50 year horizon
%	\item Examples of existing literature
%\end{itemize}


\section{Model and methodology}
\label{sec:methods}


In this paper, we used the Future Technology Transformations for the power sector model (FTT:Power) \cite{Mercure2012}. This model represents global power systems based on market competition, induced technological change and natural resource use and depletion. Induced technological change occurs through technological learning produced by cumulative investment and leads to nonlinear path dependent technological transitions \cite{Mercure2012}. The model uses a dynamic set of logistic differential equations for competition between technology options.

For this work we modified the FTT:Power model to use the deep reinforcement learning investment algorithm, DDPG. That is, the size of the investment made in each technology was made by the DDPG algorithm. In addition, we reduced the model to only consider the countries of Ireland and the UK. This enabled us to iterate through enough episodes for the reinforcement learning to converge to an optimal reward.

\subsection*{Reinforcement Learning}

The investment decision-making process can be formulated as a Markov Decision Process (MDP) \cite{puterman2014markov}. In an MDP environment, an agent receives an observation about the state of their environment $s_t$, chooses an action $a_t$ and receives a reward $r_t$ based upon their environment and action. Solving an MDP consists of maximizing the cumulative reward over the lifetime of the agent. 


For our simulation environment, the agent makes a continuous investment decision for each energy technology, in each region and each year, starting from 2017 until 2050. 	Technology switching is modelled using a pairwise comparison of flows of market shares of different electricity generation capacity. That is, how much capacity flows from one technology to another. 

The agent's observation space is a matrix consisting of the electricity produced by each technology, total capacity, total \ch{CO2} emissions over the simulation, levelized cost of electricity (LCOE) including both taxes and without taxes, cumulative investment in each technology, investment in new capacity, carrier prices by commodity, fuel costs and carbon costs.

The reward $r$ is defined as:
\begin{equation}
\label{eq:reward_function}
	r = -\left(1000\times\ch{CO2}_e + \frac{LCOE}{1000}\right),
\end{equation}

where $\ch{CO2}_e$ is equal to total \ch{CO2} emissions over the simulation, and $LCOE$ is equal to LCOE, excluding taxes. The scaling factors were used to place the $LCOE$ and $\ch{CO2}$ on the same scale. The reward was multiplied by -1 due to the RL algorithm maximizing reward and our requirement to reduce both LCOE and \ch{CO2} emissions.


RL approaches have been used to solve MDP \cite{Sutton2015}. In recent times, however, RL has been extended to incorporate Deep Reinforcement Learning (DRL). DRL relies on deep neural networks to overcome the problems of memory complexity and computation complexity \cite{Arulkumaran2017}. 

We applied the deep deterministic policy gradient (DDPG) DRL algorithm \cite{Hunt2016a} from the Ray RLlib package \cite{Liang2014}. The DDPG algorithm is made up of an actor and critic network. We designed both of these to have two hidden layers, made up of 400 and 300 units per layer. The training batch size was set to 40,000. We chose these parameters due to them being the default implementation in Ray RLlib. We trialled a variety of different configurations for number of neurons per layer, however found that the approach worked well irrespective of parameter choice, as shown in Figure \ref{fig:hyperparameter_training}. 





\begin{figure}
\centering
\includegraphics[width=0.7\columnwidth]{figures/hyperparameter_training.pdf}
\caption{Training with different hyperparameters, displaying the minimum, mean and maximum rewards per episode.}
\label{fig:hyperparameter_training}
\end{figure}



% Sample batch size = 10000
% Train batch size = 40000

% EndYear = 2050
% N = 149

% observations = [G_cum, U_cum, E_cum, CF_cum, LCOE_cum, TLCOE_cum, W_cum, I_cum, P_cum, Fcosts_cum, CO2_costs_cum];

% Electricity produced by technology
% Total capacity
% Emissions of CO2 during year
% Capacity factor
% LCOE excluding taxes and carbon price
% Levelised cost including taxes
% Cumulative investment (global)
% Investment in new capacity
% P_cum: Carrier prices by commodity
% Fcosts_cum: Fuel costs
% C02_costs_cum: Carbon costs

%
%\begin{itemize}
%	\item Use of simplified FTT:Power model (Ireland + UK)
%	\item Explain DDPG algorithm
%	\item Detail how we adopted the algorithm to minimize both cost and carbon emissions
%\end{itemize}



\section{Results}
\label{sec:results}


%\begin{figure*}
%\includegraphics[width=0.8\columnwidth]{figures/electricity_generated_plot.pdf}
%\label{fig:electricity_generated_plot}
%\caption{Mean outputs of various technologies vs. mean absolute error from 2018 to 2035 in ElecSim.}
%\end{figure*}
%
%\begin{figure*}
%\includegraphics[width=0.8\columnwidth]{figures/emissions_plot.pdf}
%\label{fig:electricity_generated_plot}
%\caption{Mean outputs of various technologies vs. mean absolute error from 2018 to 2035 in ElecSim.}
%\end{figure*}

Our results show that our investment agent is able to increase its reward over time, as shown in Figure \ref{fig:days_reward_plot}. A total of ${\sim}$400,000 steps were required to see a levelling off in reward. The total time taken to simulate ${\sim}$400,000 steps was ${\sim}$8 days. We stopped the training and simulation after this time due to diminishing returns and the cost of computation.

Figure \ref{fig:electricity_generated_plot} displays the results of reinforcement learning algorithm. Before the black vertical line (2017), the investments made are based upon historical data used by FTT:Power. The reinforcement learning algorithm starts to make investments after the black vertical line. 


The historical electricity mix before 2017 is largely based on fossil fuels: coal, combined cycle gas turbine (CCGT) and oil. Additionally, nuclear is a major component of the electricity mix before 2009. After reinforcement learning optimizes for LCOE and carbon emissions, a rapid transition occurs from fossil fuel and nuclear to renewable energy. 

This rapid transition occurs due to the reinforcement learning algorithm not taking into account the technical and timeframe constraints embedded in the unmodified FTT:Power model. However, whilst it is likely that whilst the transition speed is unrealistic, the electricity mix found by the reinforcement learning algorithm is likely to be optimal, according to the reward function defined in Equation \ref{eq:reward_function}.



%The speed of the uptake of technology adoption in FTT:Power depends on investors' preferences (Fij), the timeframes embedded in the matrix Aij (A(i,j,k) in the original code) and the technical constraints (FF, Gmax and Gmin). These elements are explicitly incorporated in the Shares equation (dSij(i,j,k)). It seems all those constraints are no longer embedded in the reinforcing learning algorithm, and that is probably the reason for having very rapid transitions. I am not familiar with the way your algorithm works, so I cannot advise you on the details. But you could potentially incorporate the aforementioned elements into the algorithm, as a proxy for adoption rate limits.


\begin{figure}[h]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/runtime_steps_plot.pdf}
  \caption{Mean, minimum and maximum rewards over run time.}
  \label{fig:days_reward_plot}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/electricity_generated_plot.pdf}
  \caption{Electricity mix over time.}
  \label{fig:electricity_generated_plot}
\end{minipage}
\end{figure}

The primary source of energy after the reinforcement learning algorithm begins is offshore, followed by onshore, solar photovoltaics (PV) and wave. As can be seen by Figure \ref{fig:emissions_plot}, the carbon emissions reduce significantly at the time that the reinforcement learning algorithm begins to control investments. 

This mix of renewable electricity generation across Ireland and the UK allows for demand to be met during the quarterly time periods of the model. The demand scenario is shown in Figure \ref{fig:demand_scenario}, where the demand can be seen to closely match the electricity mix shown by Figure \ref{fig:electricity_generated_plot}.






\begin{figure}[h]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/emissions_plot.pdf}
  \caption{Carbon emissions.}
  \label{fig:emissions_plot}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/demand_plot.pdf}
  \caption{Demand scenario over simulation.}
  \label{fig:demand_scenario}
\end{minipage}
\end{figure}





%\begin{itemize}
%	\item Display electricity mix over time-horizon
%	\item Investment in solar, offshore, onshore and wave
%	\item Visualize carbon emissions and electricity price over time
%	\item Discuss these results and detail 
%\end{itemize}


\section{Discussion}

A transition from a high carbon-emitting electricity grid to a low-carbon system is required. In order to achieve this, investments in electricity generators must be made whilst taking into account future uncertainty. In this paper, we have modelled a central agent which makes investment decisions in an uncertain environment to find an optimal low-cost, low-carbon electricity mix. To achieve this, we used the reinforcement learning algorithm, DDPG. The environment is modelled using FTT:Power.

Through this exercise, we are able to see the optimal electricity mix in the UK and Ireland. We found that a mixture of renewable sources such as wind, solar and wave power would meet demand at quarter year intervals, as well as providing a cost-effective and low-carbon system.

A limitation of this work is the fact that the investment algorithm does not take into account the technical and timeframe constraints of transitions between technologies. It is for this reason that the reinforcement learning algorithm is able to make such a rapid transition in 2017. However, we believe that the investment algorithm is able to find a general solution to the problem of investing in a cost-efficient and low-carbon system over a long time horizon.

In future work, we would like to increase the number of steps of the FTT:Power model to more adequately model the investment behaviour introduced by the reinforcement learning algorithm. A lower number of simulated time steps leads to an overestimation of the supply of renewables and underestimation of storage and dispatchable technologies \cite{Ludig2011}. In addition, an increase in the number of countries modelled would enable us to see a global picture of how different, interdependent regions may evolve in a new climate of a requirement of low-carbon emissions. This would require an exponentially longer runtime for the reinforcement learning algorithm to converge. This is due to the increased number of decisions that the reinforcement learning algorithm would need to make to account for the different countries. Finally, we would like to incorporate the technical and timeframe constraints for technology switching. This could be undertaken by modifying the reward function to ensure the transition remains within these constraints.




\section{Acknowledgements}

Acknowledgements redacted to maintain anonymity for double-blind. 




\bibliographystyle{ieeetr}
\bibliography{library,ftt-power-custom}

\end{document}
